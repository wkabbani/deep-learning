{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tips & Tricks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Practical tips for backpropagation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1. Use ReLU as the non-linear activation function\n",
    "ReLU works best for networks with many layers, which has caused alternatives like the sigmoid function and hyperbolic tangent \\tanh(\\cdot)tanh(⋅) function to fall out of favour. The reason ReLU works best is likely due to its single kink which makes it scale equivariant."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2. Use cross-entropy loss as the objective function for classification problems\n",
    "Log softmax, which we discussed earlier in the lecture, is a special case of cross-entropy loss. In PyTorch, be sure to provide the cross-entropy loss function with log softmax as input (as opposed to normal softmax)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3. Use stochastic gradient descent on minibatches during training\n",
    "As discussed previously, minibatches let you train more efficiently because there is redundancy in the data; you shouldn’t need to make a prediction and calculate the loss on every single observation at every single step to estimate the gradient."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 4. Shuffle the order of the training examples when using stochastic gradient descent\n",
    "Order matters. If the model sees only examples from a single class during each training step, then it will learn to predict that class without learning why it ought to be predicting that class. For example, if you were trying to classify digits from the MNIST dataset and the data was unshuffled, the bias parameters in the last layer would simply always predict zero, then adapt to always predict one, then two, etc. Ideally, you should have samples from every class in every minibatch."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5. Normalize the inputs to have zero mean and unit variance\n",
    "Before training, it’s useful to normalize each input feature so that it has a mean of zero and a standard deviation of one."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 6. Use a schedule to decrease the learning rate\n",
    "The learning rate should fall as training goes on. In practice, most advanced models are trained by using algorithms like Adam which adapt the learning rate instead of simple SGD with a constant learning rate.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 7. Use L1 and/or L2 regularization for weight decay\n",
    "You can add a cost for large weights to the cost function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8. Weight initialisation\n",
    "The weights need to be initialised at random, however, they shouldn’t be too large or too small such that output is roughly of the same variance as that of input. There are various weight initialisation tricks built into PyTorch. One of the tricks that works well for deep models is Kaiming initialisation where the variance of the weights is inversely proportional to square root of number of inputs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 9. Use dropout\n",
    "Dropout is another form of regularization. It can be thought of as another layer of the neural net: it takes inputs, randomly sets n/2n/2 of the inputs to zero, and returns the result as output. This forces the system to take information from all input units rather than becoming overly reliant on a small number of input units thus distributing the information across all of the units in a layer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}